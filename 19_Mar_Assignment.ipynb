{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Min-Max scaling is a technique used in data preprocessing to scale features to a specific range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the range of the feature (the difference between the maximum and minimum values). This transformation ensures that all features have the same scale, preventing certain features from dominating others in algorithms that are sensitive to feature magnitudes, such as gradient descent-based optimization algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature value,\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature,\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature, and\n",
    "- \\( X_{\\text{scaled}} \\) is the scaled feature value.\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of house prices with a feature representing the square footage of each house. Let's say the square footage ranges from 800 square feet to 2000 square feet in the dataset.\n",
    "\n",
    "- House A: Square footage = 1200 sq. ft.\n",
    "- House B: Square footage = 1600 sq. ft.\n",
    "- House C: Square footage = 2000 sq. ft.\n",
    "\n",
    "Using Min-Max scaling, you would scale these values to a range between 0 and 1:\n",
    "\n",
    "- House A: \\( \\frac{1200 - 800}{2000 - 800} = \\frac{400}{1200} = \\frac{1}{3} \\)\n",
    "- House B: \\( \\frac{1600 - 800}{2000 - 800} = \\frac{800}{1200} = \\frac{2}{3} \\)\n",
    "- House C: \\( \\frac{2000 - 800}{2000 - 800} = \\frac{1200}{1200} = 1 \\)\n",
    "\n",
    "So, after Min-Max scaling:\n",
    "- House A: Square footage = \\( \\frac{1}{3} \\)\n",
    "- House B: Square footage = \\( \\frac{2}{3} \\)\n",
    "- House C: Square footage = 1\n",
    "\n",
    "Now, all square footage values are within the range [0, 1], making them suitable for use in algorithms that require scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The Unit Vector technique, also known as vector normalization, is a feature scaling method that transforms the features in such a way that each feature vector has a Euclidean length of 1. This means that after normalization, each data point (or feature vector) lies on the surface of a unit hypersphere.\n",
    "\n",
    "The formula for unit vector normalization is:\n",
    "\n",
    "\\[ X_{\\text{normalized}} = \\frac{X}{\\|X\\|} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature vector,\n",
    "- \\( \\|X\\| \\) denotes the Euclidean norm or magnitude of the feature vector \\( X \\), and\n",
    "- \\( X_{\\text{normalized}} \\) is the normalized feature vector.\n",
    "\n",
    "Unit vector normalization scales the features while preserving the direction of the data points. It is particularly useful when the direction of the data points is more important than their magnitudes.\n",
    "\n",
    "Here's an example to illustrate unit vector normalization:\n",
    "\n",
    "Suppose you have a dataset of houses with two features: square footage and number of bedrooms. Each data point represents a house.\n",
    "\n",
    "- House A: Square footage = 1200 sq. ft., Number of bedrooms = 3\n",
    "- House B: Square footage = 1600 sq. ft., Number of bedrooms = 2\n",
    "\n",
    "To normalize the features using unit vector normalization:\n",
    "\n",
    "1. Calculate the Euclidean length of each feature vector:\n",
    "   - For House A: \\( \\|X\\| = \\sqrt{1200^2 + 3^2} \\)\n",
    "   - For House B: \\( \\|X\\| = \\sqrt{1600^2 + 2^2} \\)\n",
    "\n",
    "2. Normalize each feature vector:\n",
    "   - For House A: \\( X_{\\text{normalized}} = \\frac{(1200, 3)}{\\sqrt{1200^2 + 3^2}} \\)\n",
    "   - For House B: \\( X_{\\text{normalized}} = \\frac{(1600, 2)}{\\sqrt{1600^2 + 2^2}} \\)\n",
    "\n",
    "After normalization, the feature vectors will have a Euclidean length of 1, indicating that they lie on the surface of a unit hypersphere. This technique ensures that the direction of the data points is preserved while scaling the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. Its main goal is to reduce the dimensionality of a dataset while preserving most of the variability present in the data. PCA achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first principal component explaining the most variance and subsequent components explaining less variance.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. **Standardization**: Standardize the features of the dataset to have a mean of 0 and a standard deviation of 1. This step is essential because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the directions (principal components) of maximum variance in the data, while the eigenvalues represent the magnitude of variance along those directions.\n",
    "\n",
    "4. **Select Principal Components**: Select the top \\( k \\) eigenvectors (principal components) corresponding to the largest eigenvalues, where \\( k \\) is the desired dimensionality of the reduced dataset.\n",
    "\n",
    "5. **Project Data onto Principal Components**: Project the original data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "PCA is commonly used in various applications, including:\n",
    "- Dimensionality reduction for visualization.\n",
    "- Noise reduction and compression.\n",
    "- Feature extraction for subsequent machine learning tasks.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset containing the following features for a set of houses: square footage, number of bedrooms, number of bathrooms, and price.\n",
    "\n",
    "1. **Standardization**: Standardize the features by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Select Principal Components**: Select the top \\( k \\) eigenvectors (principal components) corresponding to the largest eigenvalues. For example, if you want to reduce the dataset to two dimensions, select the top two eigenvectors.\n",
    "\n",
    "5. **Project Data onto Principal Components**: Project the original data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "After applying PCA, you will have a reduced-dimensional representation of the dataset, which contains fewer features while preserving most of the variability present in the original data. This reduced representation can be used for further analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. PCA and feature extraction are closely related concepts, with PCA being a specific technique commonly used for feature extraction. Feature extraction refers to the process of transforming raw input data into a new set of features that capture the essential characteristics of the original data while reducing redundancy and dimensionality.\n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features into a new set of orthogonal features called principal components. These principal components are linear combinations of the original features and are chosen to capture the maximum variance present in the data. By selecting a subset of the principal components, PCA effectively extracts the most informative features from the original dataset while discarding less important ones.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Standardization**: Standardize the features of the dataset to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Select Principal Components**: Select the top \\( k \\) eigenvectors (principal components) corresponding to the largest eigenvalues, where \\( k \\) is the desired dimensionality of the reduced feature space.\n",
    "\n",
    "5. **Project Data onto Principal Components**: Project the original data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "The resulting reduced-dimensional representation contains the most informative features extracted from the original dataset. These extracted features can then be used for further analysis or modeling.\n",
    "\n",
    "Here's an example to illustrate PCA's use for feature extraction:\n",
    "\n",
    "Suppose you have a dataset containing various physical measurements of fruits, such as weight, length, width, and height. You want to extract the most important features from this dataset to classify the fruits into different categories (e.g., apple, orange, banana).\n",
    "\n",
    "1. **Standardization**: Standardize the features of the dataset to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Select Principal Components**: Select the top \\( k \\) eigenvectors (principal components) corresponding to the largest eigenvalues, where \\( k \\) is the desired dimensionality of the reduced feature space.\n",
    "\n",
    "5. **Project Data onto Principal Components**: Project the original data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "The resulting reduced-dimensional representation contains the most informative features extracted from the original dataset, which can then be used for fruit classification tasks. These extracted features may represent combinations of the original physical measurements that capture the most significant variability in the data, such as overall size and shape characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1. **Understand the Data**: First, understand the nature and range of each feature in the dataset. In this case, you have features such as price, rating, and delivery time.\n",
    "\n",
    "2. **Min-Max Scaling**: Apply Min-Max scaling to each feature individually to scale them to a range between 0 and 1. This ensures that all features have the same scale and prevents certain features from dominating others during the recommendation process.\n",
    "\n",
    "   The formula for Min-Max scaling is:\n",
    "\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "   where:\n",
    "   - \\( X \\) is the original feature value,\n",
    "   - \\( X_{\\text{min}} \\) is the minimum value of the feature,\n",
    "   - \\( X_{\\text{max}} \\) is the maximum value of the feature, and\n",
    "   - \\( X_{\\text{scaled}} \\) is the scaled feature value.\n",
    "\n",
    "3. **Example Application**:\n",
    "\n",
    "   Let's say you have the following data for a set of food delivery services:\n",
    "   - Price: $5 - $30\n",
    "   - Rating: 2.5 - 5.0\n",
    "   - Delivery Time: 10 minutes - 60 minutes\n",
    "\n",
    "   To apply Min-Max scaling:\n",
    "   - For the Price feature, you would subtract the minimum price ($5) from each price value and then divide by the range of prices ($30 - $5 = $25).\n",
    "   - For the Rating feature, you would subtract the minimum rating (2.5) from each rating value and then divide by the range of ratings (5.0 - 2.5 = 2.5).\n",
    "   - For the Delivery Time feature, you would subtract the minimum delivery time (10 minutes) from each delivery time value and then divide by the range of delivery times (60 minutes - 10 minutes = 50 minutes).\n",
    "\n",
    "   After applying Min-Max scaling, all features will be scaled to a range between 0 and 1, making them suitable for use in building the recommendation system.\n",
    "\n",
    "4. **Normalization**: Once Min-Max scaling is applied, you may also choose to normalize the scaled features to ensure that they have zero mean and unit variance. This step can further improve the performance of certain algorithms.\n",
    "\n",
    "By using Min-Max scaling to preprocess the data, you ensure that the features are on a consistent scale, making them suitable for building a recommendation system that takes into account factors such as price, rating, and delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. To use PCA for reducing the dimensionality of the dataset in a project aimed at predicting stock prices, you would follow these steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Clean the dataset by handling missing values, outliers, and any other data preprocessing steps necessary.\n",
    "   - Standardize the features to ensure that they have a mean of 0 and a standard deviation of 1. PCA is sensitive to the scale of the features, so standardization is essential.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "   - Once the data is preprocessed, apply PCA to the standardized feature matrix.\n",
    "   - Compute the covariance matrix of the standardized features.\n",
    "   - Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "   - Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the largest eigenvalues (variance) contain the most information about the dataset and are referred to as the principal components.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   - Determine the number of principal components to retain. This can be based on the cumulative explained variance ratio or a predetermined number of components.\n",
    "   - Choose the top \\( k \\) eigenvectors (principal components) corresponding to the largest eigenvalues, where \\( k \\) is the desired reduced dimensionality of the dataset.\n",
    "\n",
    "4. **Project Data onto Principal Components**:\n",
    "   - Project the original data onto the selected principal components to obtain the reduced-dimensional representation of the dataset.\n",
    "\n",
    "5. **Model Building**:\n",
    "   - Use the reduced-dimensional dataset as input for building predictive models to forecast stock prices.\n",
    "   - Train the models on historical data and evaluate their performance using appropriate evaluation metrics such as mean squared error (MSE), root mean squared error (RMSE), or others.\n",
    "\n",
    "6. **Back Transformation** (Optional):\n",
    "   - If needed, back-transform the predictions obtained from the reduced-dimensional dataset back to the original feature space for interpretation.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, you can achieve several benefits:\n",
    "- Reducing the computational complexity of modeling, especially when dealing with a large number of features.\n",
    "- Mitigating the curse of dimensionality, which can lead to overfitting and poor generalization performance.\n",
    "- Identifying the most important features that contribute to the variance in the dataset, potentially improving model interpretability.\n",
    "\n",
    "In the context of predicting stock prices, PCA can help extract the most relevant information from a large set of features, including company financial data and market trends, while reducing noise and redundancy. This can lead to more efficient and accurate predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Min-Max scaled data (-1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new minimum and maximum values\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the scaled values using Min-Max scaling formula\n",
    "scaled_data = (data - np.min(data)) / (np.max(data) - np.min(data)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Min-Max scaled data (-1 to 1):\", scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. To perform feature extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], we need to determine the number of principal components to retain. Here's how we can approach it:\n",
    "\n",
    "1. **Standardization**: Standardize the features to have zero mean and unit variance. This step is crucial for PCA.\n",
    "\n",
    "2. **Apply PCA**: Apply PCA to the standardized feature matrix.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. **Explained Variance Ratio**: Calculate the explained variance ratio for each principal component. The explained variance ratio represents the proportion of the dataset's variance explained by each principal component.\n",
    "\n",
    "5. **Cumulative Explained Variance**: Calculate the cumulative explained variance by summing up the explained variance ratios. This helps us understand how much variance in the original data is retained as we increase the number of principal components.\n",
    "\n",
    "6. **Select the Number of Principal Components**: Choose the number of principal components to retain based on the cumulative explained variance. A common heuristic is to retain enough principal components to capture a significant portion of the total variance, typically around 70-95%.\n",
    "\n",
    "Without knowing the specifics of the dataset, it's challenging to determine the exact number of principal components to retain. However, as a general guideline:\n",
    "\n",
    "- If the dataset is small or has a small number of features, you might retain most or all of the principal components to preserve as much information as possible.\n",
    "- If the dataset is large or has many features, you might aim to retain enough principal components to capture a high percentage of the total variance while reducing dimensionality.\n",
    "\n",
    "You can experiment with different numbers of principal components and evaluate their performance on a validation set or using cross-validation techniques.\n",
    "\n",
    "In practice, it's common to start by retaining a sufficient number of principal components to capture a high percentage of the total variance (e.g., 90%) and then fine-tune the number of components based on performance metrics and computational considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
