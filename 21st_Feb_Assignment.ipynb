{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ad3989-7940-4bdc-98b1-cf9208eff29c",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a332a1-cad9-4086-85b4-60f09232e1a7",
   "metadata": {},
   "source": [
    "Ans. Web scraping (or data scraping) is a technique used to collect content and data from the internet. This data is usually saved in a local file so that it can be manipulated and analyzed as needed.\n",
    "\n",
    "Web scraping has countless applications, especially within the field of data analytics. Market research companies use scrapers to pull data from social media or online forums for things like customer sentiment analysis. Others scrape data from product sites like Amazon or eBay to support competitor analysis. Meanwhile, Google regularly uses web scraping to analyze, rank, and index their content. Web scraping also allows them to extract information from third-party websites before redirecting it to their own (for instance, they scrape e-commerce sites to populate Google Shopping).\n",
    "\n",
    "Three areas where web scrapping is used to get data:\n",
    "- Search engine bots crawling a site, analyzing its content and then ranking it.\n",
    "- Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
    "- Market research companies using scrapers to pull data from forums and social media (e.g., for sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1a477-fe6f-401c-8ef9-0e049050d92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a14834-add2-4674-9be1-287b4717f9a2",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e71a78-2cc0-49ee-a6b5-7b96fe3f57e7",
   "metadata": {},
   "source": [
    "Ans. Different methods used for Web Scrapping\n",
    "- **HTML Parsing**: HTML parsing involves the use of JavaScript to target a linear or nested HTML page. It is a powerful and fast method for extracting text and links (e.g. a nested link or email address), scraping screens and pulling resources.\n",
    "- **DOM Parsing**: The Document Object Model (DOM) defines the structure, style and content of an XML file. Scrapers typically use a DOM parser to view the structure of web pages in depth. DOM parsers can be used to access the nodes that contain information and scrape the web page with tools like XPath. For dynamically generated content, scrapers can embed web browsers like Firefox and Internet Explorer to extract whole web pages (or parts of them).\n",
    "- **Vertical Aggregation**: Companies that use extensive computing power can create vertical aggregation platforms to target particular verticals. These are data harvesting platforms that can be run on the cloud and are used to automatically generate and monitor bots for certain verticals with minimal human intervention. Bots are generated according to the information required to each vertical, and their efficiency is determined by the quality of data they extract.\n",
    "- **XPath**: XPath is short for XML Path Language, which is a query language for XML documents. XML documents have tree-like structures, so scrapers can use XPath to navigate through them by selecting nodes according to various parameters. A scraper may combine DOM parsing with XPath to extract whole web pages and publish them on a destination site.\n",
    "- **Google Sheets**: Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. This command also makes it possible to check if a website can be scraped or is protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863c3d0-2892-41a8-abf8-1bb735f95936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7464d7fc-c329-4148-a70d-3e135ae58618",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4719f9-2a06-4981-bfda-3e2b14b7a8ae",
   "metadata": {},
   "source": [
    "Ans. Beautiful Soup is a python package and as the name suggests, parses the unwanted data and helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversible XML structures. In short, Beautiful Soup is a python package which allows us to pull data out of HTML and XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf3aa5-899f-462f-a257-09a17bb5460e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a9384b-10ff-4c0c-9c44-bd406928df73",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73c47b-0666-4903-9f01-e12491c24fd5",
   "metadata": {},
   "source": [
    "Ans. Flask is a lightweight framework to build websites. Its is used to parse collected data and display it as HTML in a new HTML file. The request module allows us to send http requests to the website we want to scrape. The route method is used to bind our python methods to the URL so that the collected data can be worked upon and the ouput can be shown in the web page itself by redenring the templates as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcf97c-1877-400a-b277-fbc7dc285e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64cc673d-9443-43f9-80f6-25ffd0f845fd",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71bfd62-05d2-4167-9d28-c5e6b69c57bd",
   "metadata": {},
   "source": [
    "Ans.\n",
    "1. **Code Pipeline**: AWS CodePipeline is a continuous delivery service that enables us to model, visualize, and automate the steps required to release your software. With AWS CodePipeline, we model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production\n",
    "2. **Elastic Bean Stalk**: Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deploymentâ€”from capacity provisioning, load balancing, and auto scaling to application health monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37927439-d80c-4171-b57e-f97326f84086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
